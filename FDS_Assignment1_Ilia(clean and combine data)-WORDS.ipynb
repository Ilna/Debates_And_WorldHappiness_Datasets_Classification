{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Data Combination and Data Cleaning"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Speech Dataset**"
   ],
   "metadata": {
    "id": "44da3690"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Load speech dataframe\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "sessions = np.arange(25, 76)\n",
    "data=[]\n",
    "\n",
    "for session in sessions:\n",
    "    directory = \"./TXT/Session \"+str(session)+\" - \"+str(1945+session)\n",
    "    for filename in os.listdir(directory):\n",
    "        f = open(os.path.join(directory, filename), encoding=\"utf8\")\n",
    "        if filename[0]==\".\": #ignore hidden files\n",
    "            continue\n",
    "        splt = filename.split(\"_\")\n",
    "        data.append([session, 1945+session, splt[0], f.read()])\n",
    "\n",
    "        \n",
    "df_speech = pd.DataFrame(data, columns=['Session','Year','ISO-alpha3 Code','Speech'])"
   ],
   "outputs": [],
   "metadata": {
    "id": "cd0b3ce3",
    "outputId": "2389f396-4af3-408f-ead4-16e595e79676"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Country-Name Dataset**"
   ],
   "metadata": {
    "id": "44da3690"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Load UNSD dataframe(basically is the country-name dataset)\n",
    "n = 16 #define the columns\n",
    "\n",
    "# Load all the data using lineterminator = '\\n' to get all the  \n",
    "# columns that are misplaced because of the ',' inside them\n",
    "unsd_df = pd.read_csv('UNSD — Methodology.csv', usecols=range(n), lineterminator='\\n')\n",
    "unsd_df"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Rename last column - remove the space(\\s)\n",
    "unsd_df.rename(columns={'Developed / Developing Countries\\r': 'Developed / Developing Countries'}, inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Complete/Combine the name of the \"Country or Area\" that was misplaced into  \n",
    "# the M49 Code column and adjust all other columns\n",
    "\n",
    "for i,j in unsd_df[\"M49 Code\"].items():\n",
    "    if(len(j)>3):\n",
    "        unsd_df.iloc[i, unsd_df.columns.get_loc('Country or Area')] += j\n",
    "        unsd_df.iloc[i, unsd_df.columns.get_loc('M49 Code')] = unsd_df.iloc[i, unsd_df.columns.get_loc('ISO-alpha2 Code')]\n",
    "        unsd_df.iloc[i, unsd_df.columns.get_loc('ISO-alpha2 Code')] = unsd_df.iloc[i, unsd_df.columns.get_loc('ISO-alpha3 Code')]\n",
    "        unsd_df.iloc[i, unsd_df.columns.get_loc('ISO-alpha3 Code')] = unsd_df.iloc[i, unsd_df.columns.get_loc('Least Developed Countries (LDC)')]\n",
    "        unsd_df.iloc[i, unsd_df.columns.get_loc('Least Developed Countries (LDC)')] = unsd_df.iloc[i, unsd_df.columns.get_loc('Land Locked Developing Countries (LLDC)')]\n",
    "        unsd_df.iloc[i, unsd_df.columns.get_loc('Land Locked Developing Countries (LLDC)')] = unsd_df.iloc[i, unsd_df.columns.get_loc('Small Island Developing States (SIDS)')]        \n",
    "        unsd_df.iloc[i, unsd_df.columns.get_loc('Small Island Developing States (SIDS)')] = unsd_df.iloc[i, unsd_df.columns.get_loc('Developed / Developing Countries')]\n",
    "        unsd_df.iloc[i, unsd_df.columns.get_loc('Developed / Developing Countries')] = \"Developing\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Merge Speech and Country-Name dataframe\n",
    "speech_and_unsd_df = pd.merge(unsd_df, df_speech, on=\"ISO-alpha3 Code\")\n",
    "\n",
    "# Select specific columns to the final Speech and Country-Name dataframe\n",
    "speech_and_countryName_df = speech_and_unsd_df[['Region Name', 'Country or Area', 'Session', 'Year', 'Speech']].copy()\n",
    "speech_and_countryName_df"
   ],
   "outputs": [],
   "metadata": {
    "id": "cde2a21b",
    "outputId": "00428c18-fc02-4953-f6d3-d1921abc52d4",
    "scrolled": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Happiness Dataset**"
   ],
   "metadata": {
    "id": "44da3690"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Load hapiness dataframe\n",
    "happinessdataframe = pd.read_excel('DataPanelWHR2021C2.xls', index_col=[0,1])\n",
    "happinessdataframe"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# happinessdataframe rename index from 'Country name' to'Country or Area'\n",
    "happinessdataframe.index.names = ['Country or Area', 'Year']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# Merge Speech-Country-Name dataframe with the Happiness dataframe \n",
    "# on the columns 'Country or Area' and 'Year' \n",
    "all_data_df = pd.merge(speech_and_countryName_df, happinessdataframe, left_on=['Country or Area','Year'], right_on=['Country or Area','Year'], right_index=True)\n",
    "\n",
    "# Create indexes on the columns 'Country or Area' and 'Year' \n",
    "all_data_df = all_data_df.set_index(['Country or Area','Year'])\n",
    "\n",
    "# Create two dataframes one for speeches tokenized, and one for speeches tokennized and FreqDist\n",
    "all_data_tokenized_df = all_data_df.copy()\n",
    "all_data_tokenized_FreqDist_df = all_data_df.copy()\n",
    "data_word_vector_df = all_data_df[[\"Speech\"]].copy()\n",
    "\n",
    "# This is the unmerged Speech dataset\n",
    "data_word_vector_df_unmerged = speech_and_countryName_df\n",
    "data_word_vector_df_unmerged =data_word_vector_df_unmerged.set_index([\"Country or Area\",\"Year\"])\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Download (in case you haven't already done so)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# import nltk\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('vader_lexicon')"
   ],
   "outputs": [],
   "metadata": {
    "id": "d49d6e45",
    "outputId": "77120fca-6e61-45a6-c938-dc6bbe16bfb9"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**OPTION 1) Run this if you want a dataframe merged with happiness**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "import string\n",
    "\n",
    "# Create all punctuation string variable\n",
    "punct = '!\"#$%&\\'()*+0123456789,’-—./:;<=>?@[\\\\]^_`{}~[\\n]'\n",
    "# Create a mapping table that will have as key the punctuation and as value an empty string\n",
    "transtab = str.maketrans(dict.fromkeys(punct, ''))\n",
    "\n",
    "# Loop through all the cells of \"Speech\" column\n",
    "for county_year_index,cell in data_word_vector_df[\"Speech\"].items():\n",
    "    # Remove all punctuations and convert the text to lowercase\n",
    "    words = word_tokenize(cell.translate(transtab).lower())\n",
    "    # Create an array that has all the words that don't give information\n",
    "    notuseful_words = stopwords.words(\"english\")\n",
    "    # Create and fill an empty array to gather all the important words of every \"Speech\" cell\n",
    "    useful_words = []\n",
    "    for w in words:\n",
    "        if (w not in notuseful_words) and (len(w) > 2):\n",
    "            useful_words.append(w)\n",
    "    # Fill the dataframe with the text of \"Speech\" for each cell\n",
    "    data_word_vector_df[\"Speech\"][county_year_index] = ' '.join(useful_words)\n"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**OPTION 2) You can run this instead of the above if you want to make a dataframe with word count from the unmerged example, it will take some minutes to finish around 2-4**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "import string\n",
    "\n",
    "# Create all punctuation string variable\n",
    "punct = '!\"#$%&\\'()*+0123456789,’-—./:;<=>?@[\\\\]^_`{}~[\\n]'\n",
    "# Create a mapping table that will have as key the punctuation and as value an empty string\n",
    "transtab = str.maketrans(dict.fromkeys(punct, ''))\n",
    "\n",
    "# Loop through all the cells of \"Speech\" column\n",
    "for county_year_index,cell in data_word_vector_df_unmerged[\"Speech\"].items():\n",
    "    # Remove all punctuations and convert the text to lowercase\n",
    "    words = word_tokenize(cell.translate(transtab).lower())\n",
    "    # Create an array that has all the words that don't give information\n",
    "    notuseful_words = stopwords.words(\"english\")\n",
    "    # Create and fill an empty array to gather all the important words of every \"Speech\" cell\n",
    "    useful_words = []\n",
    "    for w in words:\n",
    "        if (w not in notuseful_words) and (len(w) > 2):\n",
    "            useful_words.append(w)\n",
    "    # Fill the dataframe with the text of \"Speech\" for each cell\n",
    "    data_word_vector_df_unmerged[\"Speech\"][county_year_index] = ' '.join(useful_words)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-9-95104d1e9512>:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_word_vector_df_unmerged[\"Speech\"][county_year_index] = ' '.join(useful_words)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "\n",
    "word_count_df = count_vect.fit_transform(data_word_vector_df[\"Speech\"])\n",
    "speechOnlyDf = pd.DataFrame(word_count_df.toarray() ,columns= count_vect.get_feature_names())\n",
    "\n",
    "word_count_df_unmerged = count_vect.fit_transform(data_word_vector_df_unmerged[\"Speech\"])\n",
    "speechOnlyDfUnmerged = pd.DataFrame.sparse.from_spmatrix(word_count_df_unmerged,columns=count_vect.get_feature_names())\n"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**This is used to create a happines and speech dataframe. It creates a dataframe with the whole speech merged with happiness**\\\n",
    "**You NEED to run this if you want the following cells to play |OR| You can skip some cells below, there are comments to find it**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "import string\n",
    "\n",
    "# Create all punctuation string variable\n",
    "punct = '!\"#$%&\\'()*+0123456789,’-—./:;<=>?@[\\\\]^_`{}~[\\n]'\n",
    "# Create a mapping table that will have as key the punctuation and as value an empty string\n",
    "transtab = str.maketrans(dict.fromkeys(punct, ''))\n",
    "\n",
    "# Loop through all the cells of \"Speech\" column\n",
    "for i,j in all_data_df[\"Speech\"].items():\n",
    "    # Remove all punctuations and convert the text to lowercase\n",
    "    words = word_tokenize(j.translate(transtab).lower())\n",
    "    # Create an array that has all the words that don't give information\n",
    "    sw = stopwords.words(\"english\")\n",
    "    # Create and fill an empty array to gather all the important words of every \"Speech\" cell\n",
    "    no_sw = []\n",
    "    for w in words:\n",
    "        if (w not in sw) and (len(w) > 2):\n",
    "            no_sw.append(w)\n",
    "    # Fill the dataframe with the tokenized \"Speech\" for each cell\n",
    "    all_data_tokenized_df[\"Speech\"][i] = no_sw\n",
    "    # Fill the dataframe with the word-count of the tokenized \"Speech\" for each cell\n",
    "    all_data_tokenized_FreqDist_df[\"Speech\"][i] = FreqDist(no_sw)"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Just some 1 visualization for better understanding and some useful keywords"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "all_data_tokenized_FreqDist_df[\"Speech\"][1].plot(20)"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Data Cleaning**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Observe the mean values for each numerical column\n",
    "all_data_tokenized_FreqDist_df.describe()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Count how many NaN values we have per column\n",
    "all_data_tokenized_FreqDist_df.isnull().sum()\n"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Here is depicted that the null values are all float64 type \n",
    "all_data_tokenized_FreqDist_df.dtypes"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Keep one of the two approaches !!!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Approach 2\n",
    "\n",
    "# Remove all NaN values\n",
    "all_data_tokenized_FreqDist_df =all_data_tokenized_FreqDist_df.dropna()\n",
    "\n",
    "all_data_tokenized_df =all_data_tokenized_df.dropna()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# The only column that we should consider if it worthy to remove duplicates is \"Session\"\n",
    "\n",
    "# Food for thought\n",
    "# It is possible that there are two sessions rows with the same session for two different countries\n",
    "\n",
    "# all_data_tokenized_FreqDist_mean_df = all_data_tokenized_FreqDist_mean_df.drop_duplicates(subset=['Session'])\n",
    "# len(all_data_tokenized_df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Print the available values in column \"Session\"\n",
    "all_data_tokenized_FreqDist_df['Session'].unique()"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Removing Outliers"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from scipy import stats\n",
    "\n",
    "# A basic way to remove outliers with Z-score\n",
    "# Reference : https://stackoverflow.com/questions/23199796/detect-and-exclude-outliers-in-pandas-data-frame\n",
    "\n",
    "# I am not sure if we should remove the outliers ????\n",
    "\n",
    "# I do not think we should remove any outliers\n",
    "\n",
    "numeric_df = all_data_tokenized_FreqDist_df[['Life Ladder', 'Log GDP per capita', 'Social support', 'Healthy life expectancy at birth', 'Freedom to make life choices', 'Generosity', 'Perceptions of corruption', 'Positive affect', 'Negative affect']].copy()\n",
    "all_data_tokenized_FreqDist_outliers_df = all_data_tokenized_FreqDist_df[(np.abs(stats.zscore(numeric_df)) < 3).all(axis=1)]\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**----SKIP HERE IF YOU DIDN'T RUN THE CELL MENTIONED ABOVE----**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "#Join to create a very nice and handy dataframe of all words with index the country and year\n",
    "countryYearWordsUnmerged = pd.DataFrame()\n",
    "countryYearWordsUnmerged = speech_and_countryName_df.join(speechOnlyDfUnmerged)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# Drop the speech column cause it contains all the info we dont need anymore\n",
    "countryYearWordsUnmerged= countryYearWordsUnmerged.drop([\"Speech\"], axis = 1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# Index by country and year\n",
    "\n",
    "countryYearWordsUnmerged.set_index([\"Country or Area\", \"Year\"],inplace=True)\n",
    "countryYearWordsUnmerged.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Region Name</th>\n",
       "      <th>Session</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aac</th>\n",
       "      <th>aachen</th>\n",
       "      <th>aacknowledged</th>\n",
       "      <th>aacrev</th>\n",
       "      <th>aadd</th>\n",
       "      <th>aadda</th>\n",
       "      <th>aaddi</th>\n",
       "      <th>...</th>\n",
       "      <th>сөйлемек</th>\n",
       "      <th>тhomson</th>\n",
       "      <th>хxi</th>\n",
       "      <th>шмс</th>\n",
       "      <th>шоп</th>\n",
       "      <th>шьа</th>\n",
       "      <th>ьол</th>\n",
       "      <th>қарекет</th>\n",
       "      <th>қылмақ</th>\n",
       "      <th>ﬂagrant</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Country or Area</th>\n",
       "      <th>Year</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Algeria</th>\n",
       "      <th>1970</th>\n",
       "      <td>Africa</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1971</th>\n",
       "      <td>Africa</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1972</th>\n",
       "      <td>Africa</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1973</th>\n",
       "      <td>Africa</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1974</th>\n",
       "      <td>Africa</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 330678 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Region Name  Session  aaa  aac  aachen  aacknowledged  \\\n",
       "Country or Area Year                                                         \n",
       "Algeria         1970      Africa       25    0    0       0              0   \n",
       "                1971      Africa       26    0    0       0              0   \n",
       "                1972      Africa       27    0    0       0              0   \n",
       "                1973      Africa       28    0    0       0              0   \n",
       "                1974      Africa       29    0    0       0              0   \n",
       "\n",
       "                      aacrev  aadd  aadda  aaddi  ...  сөйлемек  тhomson  хxi  \\\n",
       "Country or Area Year                              ...                           \n",
       "Algeria         1970       0     0      0      0  ...         0        0    0   \n",
       "                1971       0     0      0      0  ...         0        0    0   \n",
       "                1972       0     0      0      0  ...         0        0    0   \n",
       "                1973       0     1      0      0  ...         0        0    0   \n",
       "                1974       0     0      0      0  ...         0        0    0   \n",
       "\n",
       "                      шмс  шоп  шьа  ьол  қарекет  қылмақ  ﬂagrant  \n",
       "Country or Area Year                                                \n",
       "Algeria         1970    0    0    0    0        0       0        0  \n",
       "                1971    0    0    0    0        0       0        0  \n",
       "                1972    0    0    0    0        0       0        0  \n",
       "                1973    0    0    0    0        0       0        0  \n",
       "                1974    0    0    0    0        0       0        0  \n",
       "\n",
       "[5 rows x 330678 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab3-Assignment1-Aux.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}